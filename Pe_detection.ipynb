{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpPldZWflnrSkWKb4M/L2K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuhaniGupta99/pulmonary_embolism_detection/blob/main/Pe_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrylybIKHifo"
      },
      "outputs": [],
      "source": [
        "!unzip rsna_subset_2500.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls rsna_subset_2500"
      ],
      "metadata": {
        "id": "5vfog5yqNCAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pydicom opencv-python albumentations"
      ],
      "metadata": {
        "id": "lHW9qP-6NB8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pydicom timm albumentations segmentation-models-pytorch seaborn"
      ],
      "metadata": {
        "id": "GTrBHRHrNB5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pylibjpeg pylibjpeg-libjpeg pylibjpeg-openjpeg gdcm"
      ],
      "metadata": {
        "id": "fZQ6MyjaNB3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "W6jGYj9klEHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, roc_curve,\n",
        "    confusion_matrix, accuracy_score,\n",
        "    precision_recall_curve, average_precision_score\n",
        ")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)\n"
      ],
      "metadata": {
        "id": "mQbNNqAgNB1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_ROOT = \"rsna_subset_2500/images\"\n",
        "LABELS_PATH = \"rsna_subset_2500/labels.csv\""
      ],
      "metadata": {
        "id": "bQZkghIPNByk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(LABELS_PATH)\n",
        "df[\"label\"] = (df[\"negative_exam_for_pe\"] == 0).astype(int)\n",
        "print(\"Total rows:\", len(df))"
      ],
      "metadata": {
        "id": "Otux5LERNBwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def series_has_images(study, series):\n",
        "    p = os.path.join(IMG_ROOT, study, series)\n",
        "    return os.path.exists(p) and len(glob(p+\"/*.dcm\")) > 0\n",
        "\n",
        "mask = [\n",
        "    series_has_images(r.StudyInstanceUID, r.SeriesInstanceUID)\n",
        "    for _, r in df.iterrows()\n",
        "]\n",
        "\n",
        "df = df[mask].reset_index(drop=True)\n",
        "print(\"Usable series:\", df.SeriesInstanceUID.nunique())\n"
      ],
      "metadata": {
        "id": "XnjrScRrNBtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "series_df = df.groupby(\"SeriesInstanceUID\")[\"label\"].first().reset_index()\n",
        "\n",
        "sns.countplot(x=\"label\", data=series_df)\n",
        "plt.title(\"PE vs Non-PE Distribution (Subset)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l-7tUULAOKTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids, val_ids = train_test_split(\n",
        "    series_df.SeriesInstanceUID,\n",
        "    test_size=0.2,\n",
        "    stratify=series_df.label,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "train_df = df[df.SeriesInstanceUID.isin(train_ids)]\n",
        "val_df   = df[df.SeriesInstanceUID.isin(val_ids)]\n",
        "\n",
        "print(\"Train series:\", train_df.SeriesInstanceUID.nunique())\n",
        "print(\"Val series:\", val_df.SeriesInstanceUID.nunique())\n"
      ],
      "metadata": {
        "id": "LfnWKnxRNBq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def window_ct(img, level=100, width=700):\n",
        "    low = level - width // 2\n",
        "    high = level + width // 2\n",
        "    img = np.clip(img, low, high)\n",
        "    img = (img - low) / (high - low)\n",
        "    return img\n"
      ],
      "metadata": {
        "id": "lsBJpeMbOv7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pydicom"
      ],
      "metadata": {
        "id": "Tf8qCgvzjbou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = train_df.iloc[0]\n",
        "sample_path = glob(\n",
        "    os.path.join(\n",
        "        IMG_ROOT,\n",
        "        sample.StudyInstanceUID,\n",
        "        sample.SeriesInstanceUID,\n",
        "        \"*.dcm\"\n",
        "    )\n",
        ")[0]\n",
        "\n",
        "dcm = pydicom.dcmread(sample_path)\n",
        "raw = dcm.pixel_array.astype(np.float32)\n",
        "raw = raw * dcm.RescaleSlope + dcm.RescaleIntercept\n",
        "win = window_ct(raw)\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,3,1); plt.imshow(raw,cmap=\"gray\"); plt.title(\"Raw CT (HU)\")\n",
        "plt.subplot(1,3,2); plt.imshow(win,cmap=\"gray\"); plt.title(\"Windowed CT\")\n",
        "plt.subplot(1,3,3); plt.hist(raw.flatten(), bins=200); plt.title(\"HU Histogram\")\n",
        "plt.tight_layout(); plt.show()\n"
      ],
      "metadata": {
        "id": "hSo2gAYpOv36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RSNADataset(Dataset):\n",
        "    def __init__(self, df, stack=2, train=True):\n",
        "        self.groups = df.groupby(\"SeriesInstanceUID\")\n",
        "        self.series_ids = list(self.groups.groups.keys())\n",
        "        self.stack = stack\n",
        "        self.train = train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.series_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sid = self.series_ids[idx]\n",
        "        g = self.groups.get_group(sid)\n",
        "        study_uid = g.StudyInstanceUID.iloc[0]\n",
        "\n",
        "        files = glob(os.path.join(IMG_ROOT, study_uid, sid, \"*.dcm\"))\n",
        "        slices = []\n",
        "\n",
        "        for f in files:\n",
        "            dcm = pydicom.dcmread(f)\n",
        "            img = dcm.pixel_array.astype(np.float32)\n",
        "            img = img * dcm.RescaleSlope + dcm.RescaleIntercept\n",
        "            z = float(dcm.ImagePositionPatient[2])\n",
        "            img = window_ct(img)\n",
        "            img = cv2.resize(img, (224,224))\n",
        "            slices.append((z, img))\n",
        "\n",
        "        # sort by physical z-position\n",
        "        slices = [s[1] for s in sorted(slices, key=lambda x: x[0])]\n",
        "        n = len(slices)\n",
        "\n",
        "        # ðŸ”¥ SAFE CENTER SELECTION\n",
        "        if n < 2 * self.stack + 1:\n",
        "            # too few slices â†’ pad by repeating center\n",
        "            center = n // 2\n",
        "            idxs = [center] * (2 * self.stack + 1)\n",
        "        else:\n",
        "            center = (\n",
        "                np.random.randint(self.stack, n - self.stack)\n",
        "                if self.train else n // 2\n",
        "            )\n",
        "            idxs = range(center - self.stack, center + self.stack + 1)\n",
        "\n",
        "        x = torch.tensor(np.stack([slices[i] for i in idxs])).unsqueeze(1)\n",
        "        y = torch.tensor(g.label.iloc[0], dtype=torch.float32)\n",
        "\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "LLeRP8IbOv1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    RSNADataset(train_df, stack=4, train=True),\n",
        "    batch_size=4, shuffle=True, num_workers=2\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    RSNADataset(val_df, stack=4, train=False),\n",
        "    batch_size=4, shuffle=False, num_workers=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "h2atOpB4OvyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnifiedPEModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = timm.create_model(\n",
        "            \"efficientnet_b2\",\n",
        "            pretrained=True,\n",
        "            in_chans=1,\n",
        "            features_only=True\n",
        "        )\n",
        "\n",
        "        C = self.encoder.feature_info[-1][\"num_chs\"]\n",
        "\n",
        "        # Slice attention\n",
        "        self.slice_attn = nn.Sequential(\n",
        "            nn.Linear(C, C//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(C//2, 1)\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(C, 1)\n",
        "        )\n",
        "\n",
        "        # Weak segmentation head\n",
        "        self.seg_head = nn.Sequential(\n",
        "            nn.Conv2d(C, C//2, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(C//2, 1, 1)\n",
        "        )\n",
        "\n",
        "        self.feature_maps = None\n",
        "        self.feature_grads = {}\n",
        "\n",
        "    def save_grad(self, idx):\n",
        "        def hook(grad):\n",
        "            self.feature_grads[idx] = grad\n",
        "        return hook\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, S, C, H, W = x.shape\n",
        "        x = x.view(B*S, C, H, W)\n",
        "\n",
        "        feats = self.encoder(x)\n",
        "\n",
        "        self.feature_maps = feats\n",
        "        self.feature_grads = {}\n",
        "\n",
        "        if torch.is_grad_enabled():\n",
        "            for i, f in enumerate(feats):\n",
        "                f.register_hook(self.save_grad(i))\n",
        "\n",
        "        feat = feats[-1].view(\n",
        "            B, S,\n",
        "            feats[-1].shape[1],\n",
        "            feats[-1].shape[2],\n",
        "            feats[-1].shape[3]\n",
        "        )\n",
        "\n",
        "        pooled = feat.mean(dim=(3,4))\n",
        "        attn = torch.softmax(self.slice_attn(pooled), dim=1)\n",
        "        feat = (feat * attn.unsqueeze(-1).unsqueeze(-1)).sum(dim=1)\n",
        "\n",
        "        cls = self.cls_head(\n",
        "            F.adaptive_avg_pool2d(feat,1).flatten(1)\n",
        "        ).squeeze(1)\n",
        "\n",
        "        seg = self.seg_head(feat)\n",
        "        seg_up = F.interpolate(seg, (224,224), mode=\"bilinear\")\n",
        "\n",
        "        return cls, seg_up\n"
      ],
      "metadata": {
        "id": "ETH4C3c9Ovvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiScaleGradCAM:\n",
        "    def __init__(self, model, scales=(1,2,3)):\n",
        "        self.model = model\n",
        "        self.scales = scales\n",
        "\n",
        "    def generate(self, x):\n",
        "        self.model.zero_grad()\n",
        "        cls,_ = self.model(x)\n",
        "        cls.mean().backward(retain_graph=True)\n",
        "\n",
        "        cams=[]\n",
        "        for i in self.scales:\n",
        "            act = self.model.feature_maps[i]\n",
        "            grad = self.model.feature_grads[i]\n",
        "            w = grad.mean(dim=(2,3),keepdim=True)\n",
        "            cam = F.relu((w*act).sum(1,keepdim=True))\n",
        "            cam = F.interpolate(cam,(224,224))\n",
        "            cams.append(cam)\n",
        "\n",
        "        cam = torch.mean(torch.stack(cams),0)\n",
        "        return cam/(cam.max()+1e-8)\n"
      ],
      "metadata": {
        "id": "TEJXyS7LO55u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timm"
      ],
      "metadata": {
        "id": "fL3Y7VMKj0zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_pos = (train_df.label == 1).sum()\n",
        "num_neg = (train_df.label == 0).sum()\n",
        "\n",
        "pos_weight = torch.tensor([num_neg / num_pos]).to(DEVICE)\n"
      ],
      "metadata": {
        "id": "VlkQKb2TnKty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "seg_loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def dice_loss(pred, target, smooth=1e-6):\n",
        "    pred = torch.sigmoid(pred)\n",
        "    inter = (pred * target).sum(dim=(1,2,3))\n",
        "    union = pred.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))\n",
        "    dice = (2 * inter + smooth) / (union + smooth)\n",
        "    return 1 - dice.mean()\n"
      ],
      "metadata": {
        "id": "T2iWgkL_dzko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UnifiedPEModel().to(DEVICE)\n",
        "cam_gen = MultiScaleGradCAM(model)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
        "scaler = torch.amp.GradScaler(\"cuda\")\n"
      ],
      "metadata": {
        "id": "1RVRl3k6ajpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    model.train()\n",
        "    total = 0\n",
        "\n",
        "    if epoch < 5:\n",
        "        seg_weight = 0.0\n",
        "    elif epoch < 15:\n",
        "        seg_weight = 0.05\n",
        "    else:\n",
        "        seg_weight = 0.1\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS} | Seg Weight: {seg_weight}\")\n",
        "\n",
        "    for x, y in tqdm(train_loader):\n",
        "\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast(\"cuda\"):\n",
        "\n",
        "            cls, seg = model(x)\n",
        "            cls_loss = cls_loss_fn(cls, y)\n",
        "\n",
        "            if seg_weight > 0:\n",
        "\n",
        "                cam = cam_gen.generate(x)\n",
        "\n",
        "                B, S = x.shape[0], x.shape[1]\n",
        "                cam = cam.view(B, S, 1, 224, 224).mean(1)\n",
        "\n",
        "                cam_min = cam.view(B, -1).min(dim=1)[0].view(B,1,1,1)\n",
        "                cam_max = cam.view(B, -1).max(dim=1)[0].view(B,1,1,1)\n",
        "                cam_n = (cam - cam_min) / (cam_max - cam_min + 1e-8)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    pseudo_mask = (cam_n > 0.35).float()\n",
        "\n",
        "                pe = y.view(-1,1,1,1)\n",
        "\n",
        "                seg_bce = seg_loss_fn(seg * pe, pseudo_mask * pe)\n",
        "                seg_dice = dice_loss(seg * pe, pseudo_mask * pe)\n",
        "\n",
        "                seg_loss = seg_bce + seg_dice\n",
        "\n",
        "                loss = cls_loss + seg_weight * seg_loss\n",
        "\n",
        "            else:\n",
        "                loss = cls_loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    train_losses.append(total / len(train_loader))\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Loss: {train_losses[-1]:.4f}\")\n"
      ],
      "metadata": {
        "id": "GS-txOfJO5mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "y_true, y_prob = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "        x = x.to(DEVICE)\n",
        "        cls, _ = model(x)\n",
        "\n",
        "        y_true.extend(y.numpy())\n",
        "        y_prob.extend(torch.sigmoid(cls).cpu().numpy())\n",
        "\n",
        "auc = roc_auc_score(y_true, y_prob)\n",
        "print(\"Validation AUROC:\", auc)\n",
        "\n",
        "y_pred = (np.array(y_prob) > 0.5).astype(int)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"Sensitivity:\", tp/(tp+fn+1e-8))\n",
        "print(\"Specificity:\", tn/(tn+fp+1e-8))\n"
      ],
      "metadata": {
        "id": "2ToduTM2PBnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = next(iter(val_loader))\n",
        "x = x.to(DEVICE)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "cls, seg = model(x)\n",
        "cam = cam_gen.generate(x)\n",
        "\n",
        "with torch.no_grad():\n",
        "    seg_out = torch.sigmoid(seg)\n",
        "\n",
        "B, S = x.shape[0], x.shape[1]\n",
        "cam = cam.view(B, S, 1, 224, 224).mean(1)\n",
        "\n",
        "plt.figure(figsize=(14,4))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(x[0,2,0].cpu(), cmap=\"gray\")\n",
        "plt.title(\"CT\")\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(cam[0,0].detach().cpu(), cmap=\"jet\")\n",
        "plt.title(\"Pseudo Mask (CAM)\")\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(seg_out[0,0].cpu(), cmap=\"jet\")\n",
        "plt.title(\"Weak Segmentation\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ke1xJByHeBnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=(np.array(y_prob)>0.5).astype(int)\n",
        "cm=confusion_matrix(y_true,y_pred)\n",
        "tn,fp,fn,tp=cm.ravel()\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_true,y_pred))\n",
        "print(\"Sensitivity:\", tp/(tp+fn+1e-8))\n",
        "print(\"Specificity:\", tn/(tn+fp+1e-8))\n",
        "\n",
        "fpr,tpr,_=roc_curve(y_true,y_prob)\n",
        "plt.plot(fpr,tpr,label=f\"AUC={auc:.3f}\")\n",
        "plt.plot([0,1],[0,1],'--')\n",
        "plt.legend(); plt.title(\"ROC Curve\"); plt.show()\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_true,(np.array(y_pred)>0.5).astype(int))\n",
        "sns.heatmap(cm,annot=True,fmt=\"d\",cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\"); plt.show()\n"
      ],
      "metadata": {
        "id": "Ez1N-_stPDNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Tty0nV44PF25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = next(iter(val_loader))\n",
        "x = x.to(DEVICE)\n",
        "\n",
        "cam = cam_gen.generate(x)\n",
        "_, det = model(x)\n",
        "det_up = F.interpolate(det, (224,224), mode=\"bilinear\")\n",
        "\n",
        "plt.figure(figsize=(14,4))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(x[0,2,0].detach().cpu(), cmap=\"gray\")\n",
        "plt.title(\"CT\")\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(cam[0,0].detach().cpu(), cmap=\"jet\")\n",
        "plt.title(\"CAM\")\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(det_up[0,0].detach().cpu(), cmap=\"jet\")\n",
        "plt.title(\"Detection\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "E-HK-urgPHnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "y_true, y_prob = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "        x = x.to(DEVICE)\n",
        "        cls, _ = model(x)\n",
        "        y_true.extend(y.numpy())\n",
        "        y_prob.extend(torch.sigmoid(cls).cpu().numpy())\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_prob = np.array(y_prob)\n"
      ],
      "metadata": {
        "id": "yg1EDG-bbC5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
        "ap = average_precision_score(y_true, y_prob)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(recall, precision, label=f\"AP = {ap:.3f}\")\n",
        "plt.xlabel(\"Recall (Sensitivity)\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precisionâ€“Recall Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5L7tocUdbDOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_losses = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    for x, y in val_loader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        cls, _ = model(x)\n",
        "        total += cls_loss_fn(cls, y).item()\n",
        "\n",
        "val_losses.append(total / len(val_loader))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label=\"Training Loss\")\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qvR5w84gbE5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iou(cam, det, thr=0.35):\n",
        "    cam_bin = cam > thr\n",
        "    det_bin = det > thr\n",
        "    inter = (cam_bin & det_bin).sum()\n",
        "    union = (cam_bin | det_bin).sum()\n",
        "    return (inter / union).item() if union > 0 else 0.0\n",
        "\n",
        "ious = []\n",
        "\n",
        "model.eval()\n",
        "for x, y in val_loader:\n",
        "    x = x.to(DEVICE)\n",
        "\n",
        "    cam = cam_gen.generate(x)\n",
        "    _, det = model(x)\n",
        "\n",
        "    B, S = x.shape[0], x.shape[1]\n",
        "    cam = cam.view(B, S, 1, 224, 224).mean(1)\n",
        "    det_up = F.interpolate(det, (224,224))\n",
        "\n",
        "    for i in range(B):\n",
        "        ious.append(iou(cam[i].detach().cpu(), det_up[i].detach().cpu()))\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(ious, bins=20)\n",
        "plt.xlabel(\"IoU (CAM vs Detection)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"CAMâ€“Detection Overlap Distribution\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "W8SAn-gGbHYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "sns.kdeplot(y_prob[y_true == 1], label=\"PE\", fill=True)\n",
        "sns.kdeplot(y_prob[y_true == 0], label=\"No PE\", fill=True)\n",
        "plt.xlabel(\"Predicted Probability\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.title(\"Prediction Score Distribution\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tb2YxNbGbJIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "thresholds = np.linspace(0, 1, 50)\n",
        "sens_list, spec_list, acc_list = [], [], []\n",
        "\n",
        "for t in thresholds:\n",
        "    preds = (y_prob > t).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()\n",
        "\n",
        "    sens_list.append(tp / (tp + fn + 1e-8))\n",
        "    spec_list.append(tn / (tn + fp + 1e-8))\n",
        "    acc_list.append((tp + tn) / (tp + tn + fp + fn))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(thresholds, sens_list, label=\"Sensitivity\")\n",
        "plt.plot(thresholds, spec_list, label=\"Specificity\")\n",
        "plt.plot(thresholds, acc_list, label=\"Accuracy\")\n",
        "plt.xlabel(\"Decision Threshold\")\n",
        "plt.ylabel(\"Metric Value\")\n",
        "plt.title(\"Threshold Sensitivity Analysis\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MpZQPTzdbKsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
        "youden = tpr - fpr\n",
        "best_idx = np.argmax(youden)\n",
        "\n",
        "best_thr = thr[best_idx]\n",
        "best_sens = tpr[best_idx]\n",
        "best_spec = 1 - fpr[best_idx]\n",
        "\n",
        "print(f\"Optimal threshold (Youden): {best_thr:.3f}\")\n",
        "print(f\"Sensitivity: {best_sens:.3f}\")\n",
        "print(f\"Specificity: {best_spec:.3f}\")\n"
      ],
      "metadata": {
        "id": "Dja4rHI5bMeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AJ40fNvSbQUB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}